# 爬虫笔记总结



## 简单获取网页源码

```python
from urllib.request import urlopen

# 输出整个网页的源代码
html = urlopen("https://mofanpy.com/static/scraping/basic-structure.html").read().decode('utf-8')
print(html)
```





## 使用BeautfulSoup

### 创建

```python
from bs4 import BeautifulSoup
from urllib.request import urlopen

html = urlopen("https://mofanpy.com/static/scraping/basic-structure.html").read().decode('utf-8')

soup = BeautifulSoup(html, 'lxml')
```



### 控制搜索文档的范围

```python
from bs4 import SoupStrainer

# 只搜索a标签
only_a_tags = SoupStrainer("a")
soup = BeautifulSoup(html, "lxml", parse_only=only_a_tags)
# 输出结果：
# <a class="sister" href="http://example.com/elsie" id="link1"> Elsie </a>
# <a class="sister" href="http://example.com/lacie" id="link2"> Lacie </a>
# ...


# 只搜索id="link2"部分
only_tags_with_id_link2 = SoupStrainer(id="link2")
soup = BeautifulSoup(html_doc, "lxml", parse_only=only_tags_with_id_link2)
# 输出结果：
# <a class="sister" href="http://example.com/lacie" id="link2"> Lacie </a>


# 只搜索字符串小于10的部分
def is_short_string(string):
    return len(string) < 10
only_short_strings = SoupStrainer(string=is_short_string)
soup = BeautifulSoup(html_doc, "lxml", parse_only=only_short_strings)
# 输出结果：
# Elsie
# ,
# Lacie
# ...
```



### find_all方法使用

```python
soup.find_all([标签名],
              [标签内置属性（a='bc'）], # 匹配内置属性
              [字符串],  # 匹配正文内容
              [limit],  # 最大返回结果，防止文档过大，搜索时间太长
              [recursive = False])  # 控制搜索是否只查找其直接子节点，一般不写此项
```

**匹配标签**

```python
soup.title  # 输出内容：<title>The Dormouse's story</title>
soup.title.string  # 输出内容：The Dormouse's story

# soup.find_all(['a', 'c', 'd'])
# soup.find_all(re.complie('b'))
findAll = soup.find_all('a')  
for i in findAll:
    print('\n', i)  # 输出内容：<a href="{{ site_url }}/">莫烦Python</a>
    print(i['href'])  # 输出内容：{{ site_url }}/
    print(i.get_text())  # 输出内容：莫烦Python
```

**匹配class**

```python
soup.find_all("a", class_="sister")  # class为python内部关键字，所以这里要用class_
# 结果：<a class="sister" id="link1">Elsie</a>

soup.find_all("p", class_="body strikeout")
# 结果：<p class="body strikeout"></p>

mouth = soup.find_all('li', attrs={'class': 'month'})
for m in mouth:
    print(m.get_text())
```

**匹配其他属性**

```python
soup.find_all(id = "link2")
# 结果：[<a class="sister" id="link2">Lacie</a>]

soup.find_all(attrs={'data-foo': 'value'})  # 创建字典来匹配一些特殊tag
# 结果：[<div data-foo="value">foo!</div>]

soup.find_all(href = re.compile("elsie"), id='link1')
```

**匹配正文内容**

```python
soup.find_all("a", string = "Elsie")
# 结果：[<a class="sister" id="link1">Elsie</a>]
```

**控制搜索个数**

```python
soup.find_all("a", limit = 2)
# 结果1：<a class="sister"...
# 结果2：<a class="sister"...
```

**单个查找**

```python
soup.find('...')
```



### select方法使用（css选择器）

**对tag查找**

```python
soup.select("title")  # 可以直接输入标签进行简单筛选
# [<title>The Dormouse's story</title>]

soup.select("body a")  # 对标签的逐层查找
# [<a class="sister" id="link1">Elsie</a>,
#  ...]

soup.select("head > title")  # 对标签的子标签进行查找
# [<title>The Dormouse's story</title>]
```

**对CSS元素查找**

```python
soup.select("p:nth-of-type(3)")  # 对P标签的nth-of-type(3)样式进行筛选
# [<p class="story">...</p>]

soup.select("p > a:nth-of-type(2)")
# [<a class="sister" id="link2">Lacie</a>]

soup.select(".sister")  # 通过CSS类名查找
# [<a class="sister" id="link1">Elsie</a>,
# ...]

soup.select("#link1")  # 通过Id来查找
# [<a class="sister" id="link1">Elsie</a>]

soup.select('a[href]')  # 通过某个属性来查找
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>
# ...

soup.select('a[href="http://example.com/elsie"]')  # 通过属性的值来查找
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>]
```

**单个查找**

```python
soup.select_one('....')
```



### get_text方法

```python
markup = '<a href="http://example.com/">\nI linked to <i>example.com</i>\n</a>'
soup = BeautifulSoup(markup)

soup.i.get_text()
# 输出结果：example.com

soup.get_text("|")  # 指定输出后，文本之间的分割符
# 输出结果：I linked to |example.com|

soup.get_text("|", strip=True)  # 去除文本前后空白
# 输出结果：I linked to|example.com
```





## 使用Requests

### GET

```python
import requests

html = 'https://www.douban.com/'
r = requests.get(html)
r.encoding = 'utf-8'

print(r.status_code)  # 输出返回值，例如：200，502，404等
print(r.text)  # 输出网页代码
```

**注意：**如果返回内容的中文存在问题，则需要使用`r.encoding`来修改编码

**带参数**

```python
html = 'https://www.douban.com/search'
param = {'q':'python', 'cat':'1001'}

r = requests.get(html, params=param)
r.url
# 运行结果：https://www.douban.com/search?q=python&cat=1001
```

**带Header**

```python
html = 'https://www.douban.com/'
header = {'User-Agent':'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit'}

r = requsets.get(html, headers=header)
r.text
# 运行结果：<!DOCTYPE html>\n<html>\n<head>\n<meta charset="UTF-8">\n <title>豆瓣(手机版)</title>...
```

**带Cookies**

```python
html = '......'
cs = {'token': '12345', 'status': 'working'}

r = requests.get(html, cookies=cs)
```



### POST

```python
import requests

html = 'http://pythonscraping.com/files/processing.php'
data = {'firstname':'user', 'lastname':'123'}

r = requests.post(html, data=data)
r.text
# 运行结果：hello there, user 123!
```

**上传文件**

```python
html = 'http://pythonscraping.com/files/processing2.php'
file = ('uploadFile': open('./image.png', 'rb'))  # rb为二进制模式读

r = requests.post(html, files=file)
```



### Session方法使用

```python
html = 'http://pythonscraping.com/pages/cookies/welcome.php'
payload = {'username': 'Guan', 'password': 'password'}

session = requests.Session()
session.post(html, data=payload)  # 提交cookies

r = session.get(html)  # 获得登录后的网页源码
print(r.text)  # 打印获取结果
```



### requests文件下载

```python
imageUrl = 'https://mofanpy.com/static/img/description/learning_step_flowchart.png'
r = requests.get(imageUrl)

with open(r'C:\Users\19657\Desktop\test.png', 'wb') as f:
    f.write(r.content)  # 全部保存到内存中再写入硬盘

# 针对大文件，可以一边下载，一边写入硬盘，防止文件太大内存放不下
with open(r'C:\Users\19657\Desktop\test.png', 'wb') as f:
    for chunk in r.iter_content(chunk_size=32):
        f.write(chunk)
```





## 使用Selenium

```python
from selenium import webdriver

driver = webdriver.Chrome()  # 创建实例
# driver.implicitly_wait(1)  # 等待1秒再操作
driver.get("https://baidu.com")  # 打开网址
```



### 元素定位

```python
'''
driver.find_element_by_[id, 
                        name, 
                        class_name, 
                        tag_name, 
                        link_text,
                        partial_link_text, 
                        xpath, 
                        css_selector]
'''

# 举例：<input type="text" name="passwd" id="passwd-id" />
element = driver.find_element_by_id("passwd-id")
element = driver.find_element_by_name("passwd")
element = driver.find_element_by_xpath("//input[@id='passwd-id']")
```



### 行为

```python
# 元素点击
element.click()  # 单击元素
```

**鼠标操作函数**

```python
from selenium.webdriver.common.action_chains import ActionChains  # 导包

move_to_element('元素')  # 移动到指定元素上
click_and_hold()  # 按住左键
context_click()  # 右击；如果没有参数，默认右击位置为鼠标当前位置
double_click()  # 双击
drag_and_drop('鼠标按下的元素', '鼠标松开的元素')  # 拖动
key_down()  # 按下修饰键(Control、Alt和Shift)
key_up()  # 松开修饰键
```

**鼠标操作举例**

```python
# 模拟鼠标移动到元素上并点击
firstClick = driver.find_element_by_xpath('...')
ActionChains(driver).move_to_element(firstClick).click().perform()

# 模拟Ctrl+c操作
ActionChains(driver).key_down(Keys.CONTROL).send_keys('c').key_up(Keys.CONTROL).perform()
```

**注意：**对于鼠标事件，最后都要使用`.perform()`来提交存储的行为才会有点击效果



### 输入

```python
from selenium.webdriver.common.keys import Keys  # 键盘映射

element.clear()  # 清除文本内容
element.send_keys("selenium")  # 模拟输入
element.submit()  # 提交内容
```

**特殊按键**

```python
element.send_keys(Keys.BACK_SPACE)  # 删除键(BackSpace)
element.send_keys(Keys.SPACE)  # 空格键(Space)
element.send_keys(Keys.TAB)  # 制表键(Tab)
element.send_keys(Keys.ESCAPE)  # 回退键(Esc)
element.send_keys(Keys.ENTER)  # 回车键(Enter)
element.send_keys(Keys.CONTROL, 'a')  # 全选(Ctrl+A)
element.send_keys(Keys.CONTROL, 'c')  # 复制(Ctrl+C)
element.send_keys(Keys.CONTROL, 'x')  # 剪切(Ctrl+X)
element.send_keys(Keys.CONTROL, 'v')  # 粘贴(Ctrl+V)
element.send_keys(Keys.F1)  # 键盘F1
element.send_keys(Keys.F12)  # 键盘F12
```



### 特殊功能

**访问前设置参数**

**提示：**关闭可视化界面程序运行速度明显变慢，因此不建议使用隐式访问

```python
opt = webdriver.ChromeOptions()  # 设置浏览器参数
opt.add_argument('window-size=1280x1080')  # 指定浏览器分辨率
opt.add_argument('--hide-scrollbars')  # 隐藏滚动条, 应对一些特殊页面
opt.add_argument('--headless')  # 浏览器不提供可视化页面
driver = webdriver.Chrome(options=opt)  # 创建实例
```

**控制浏览器窗口大小**

**注意：**如果是隐式访问，则此语句对浏览器页面无效，解决方法是使用参数式设置浏览器分辨率。设置语句见上面：`访问前设置参数`

```python
driver.set_window_size(1280, 1080)
```

**控制浏览器后退 和 前进**

```python
driver.back()
driver.forward()
```

**刷新**

```python
driver.refresh()
```

**关闭**

```python
driver.close()  # 关闭单个窗口
driver.quit()  # 关闭所有窗口
```

**对当前页面截图**

```python
driver.get_screenshot_as_file("D:\\baidu_img.jpg")
```

**切换窗口**

```python
search_windows = driver.current_window_handle  # 获取当前页面的句柄
all_handles = driver.window_handles  # 获取所有页面的句柄

for handle in all_handles:
    if handle != search_windows:
		driver.switch_to.window(handle)  # 循环切换窗口

driver.switch_to.window(driver.window_handles[-1]) #  切换到最新打开窗口
```





## 使用Scrapy

### 创建项目

```powershell
# 在cmd或者powershell中使用scrapy命令创建一个新项目
scrapy startproject 项目名

# 创建的目录结构为：
# 项目名
# └-- 项目名
#     └-- spiders (存放爬虫项目的文件夹)
#         └-- _init_.py
#     └-- _init_.py
#     └-- items.py (保存爬取到的数据的容器)
#     └-- middlewares.py (中间件)
#     └-- pipelines.py (管道文件，负责处理爬取到的信息)
#     └-- settings.py (项目的设置文件)
# └-- scrapy.cfg (项目的配置文件)
```

**更多内容：**https://scrapy-chs.readthedocs.io/zh_CN/0.24/index.html



### 定义Item

在items.py中写好要获取的数据

```python
import scrapy

class Class8Item(scrapy.Item):
    name = scrapy.Field()
    link = scrapy.Field()
```



### 编写爬虫文件

在终端中使用命令创建爬虫文件：

```powershell
scrapy genspider 爬虫名 爬虫网址
```

如果自己创建文件，类继承`scrapy.Spider`时，要实现三个属性，分别是下面的：`name`、`start_urls`、`parse()`

```python
import re
import scrapy
from class_8.items import Class8Item  # 此处报红为正常现象，无需修改，能正常出结果


class DoubanSpider(scrapy.Spider):
    name = "douban"  # 爬虫名字，在当前项目中应该是唯一的，不能重复
    start_urls = ["https://..."]  # 要爬取的网站

        def parse(self, response, **kwargs):
        for li in response.xpath('//...//li'):
            content = Class8Item()
            # 此处xpath中开头的点不要忽略，它的效果是在上一个的基础上继续匹配
            content["name"] = li.xpath('.//.../text()').extract_first()
            link = li.xpath('.//img/@src').extract_first()
            content["link"] = re.sub('.jpg', '.webp', link)
            print(content["name"])
            yield content

        # 跳转下一页
        next_page_url = response.urljoin(
            response.xpath('...a/@href').extract_first()
        )
        if next_page_url is not None:
            yield scrapy.Request(
                next_page_url,  # 翻页的url
                callback=self.parse,  # callback:指定传入的url参数用哪个函数去解析
                # meta=...  # 用它在不同的解析函数中传递数据
                # dont_filter=[False|True]  # 让scrapy选择是否对翻页的url进行去重，默认是有去重功能
            )

'''
Selector有四个基本的方法：
xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表
css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表
extract(): 序列化该节点为unicode字符串并返回list
re(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表
'''
```

编写完后可以在终端中输入命令：`scrapy crawl douban`来运行刚才创建的爬虫文件（命令中的`douban`是文件中自己定义的爬虫名字），还可以使用`-o`参数将结果输出到文件中，例如：`scrapy crawl douban -o douban.json`



### 下载图片文件

```python
import scrapy
from scrapy.pipelines.images import ImagesPipeline


class Class8Pipeline(ImagesPipeline):
    #@override 对图片地址发起请求
    def get_media_requests(self, item, info):
        yield scrapy.Request(item["link"], meta={"item": item})

    # 图片的文件名
    def file_path(self, request, response=None, info=None, *, item=None):
        item = request.meta["item"]
        file_name = item["number"] + "_" + item["name"] + ".jpg"
        return file_name  # 格式：1_xxx.jpg

    #@override 当一个单独项目中的所有图片请求完成时（要么完成下载，要么因为某种原因下载失败）将被调用
    def item_completed(self, results, item, info):
        return item
```

**注意：**使用`scrapy`自带的图片下载需要先用`pip`安装`Pillow`，对于部分网站，可能会出现程序运行但是没有下载图片，这是因为图片资源可能被网站的爬虫协议列为禁止下载，此时需要将设置文件中的`ROBOTSTXT_OBEY`设置为`False`，表示不遵从爬虫协议。



### 调整设置文件

`setting.py`文件控制爬虫的设置，比如`log`等级、编码等

```python
# 需要自己增加的内容
LOG_LEVEL = "WARNING"  # 将log等级设置为WARNING及以上，这样控制台就只有WARNING以上的信息会显示
FEED_EXPORT_ENCODING = "utf-8"  # 设置编码为utf-8，当需要将中文写入文件时，需要此设置

MYEXT_ENABLED = True  # 开启setting的自定义插件
CLOSESPIDER_TIMEOUT = 0  # 超过指定的秒数后自动关闭
CLOSESPIDER_PAGECOUNT = 0  # 超过指定的抓取响应(reponses)数后自动关闭
CLOSESPIDER_ITEMCOUNT = 0  # 超过指定的抓取数量后自动关闭
CLOSESPIDER_ERRORCOUNT = 0  # 超过指定的错误数目后自动关闭

# 直接在文件中修改的内容
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\
             'AppleWebKit/537.36 (KHTML, like Gecko) '\
             'Chrome/88.0.4324.150 Safari/537.36'

ROBOTSTXT_OBEY = True  # 是否遵循爬虫协议，如果需要下载东西，则要改成False，否则大概率下载不到内容
CONCURRENT_REQUESTS = 32  # 下载的并行数量，默认是16

IMAGES_STORE = r'C:\Users\19657\Desktop\test'  # 下载文件的保存路径
ITEM_PIPELINES = {
   'class_8.pipelines.Class8Pipeline': 300,  # pipelines.py文件用到
}

CLOSESPIDER_PAGECOUNT = 5
EXTENSIONS = {
    'scrapy.extensions.closespider.CloseSpider' : 500, #是否开启自动关闭条件
}
```





## XPath规则

| 表达式             | 作用                             |
| ------------------ | -------------------------------- |
| nodename(节点名称) | 选取此节点的所有子节点           |
| /                  | 从当前节点选取直接子节点         |
| //                 | 从当前节点选取子孙节点           |
| .                  | 选取当前节点                     |
| ..                 | 选取当前节点的父节点             |
| @                  | 选取属性                         |
| []                 | 下标，控制选取第几个，从1开始    |
| text()             | 标签的内容，即网页显示的文本部分 |

| 运算符 | 描述           | 实例                      | 返回值                                                       |
| :----- | :------------- | :------------------------ | :----------------------------------------------------------- |
| \|     | 计算两个节点集 | //book \| //cd            | 返回所有拥有 book 和 cd 元素的节点集                         |
| +      | 加法           | 6 + 4                     | 10                                                           |
| -      | 减法           | 6 - 4                     | 2                                                            |
| *      | 乘法           | 6 * 4                     | 24                                                           |
| div    | 除法           | 8 div 4                   | 2                                                            |
| =      | 等于           | price=9.80                | 如果 price 是 9.80，则返回 true。如果 price 是 9.90，则返回 false。 |
| !=     | 不等于         | price!=9.80               | 如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。 |
| <      | 小于           | price<9.80                | 如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。 |
| <=     | 小于或等于     | price<=9.80               | 如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。 |
| >      | 大于           | price>9.80                | 如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。 |
| >=     | 大于或等于     | price>=9.80               | 如果 price 是 9.90，则返回 true。如果 price 是 9.70，则返回 false。 |
| or     | 或             | price=9.80 or price=9.70  | 如果 price 是 9.80，则返回 true。如果 price 是 9.50，则返回 false。 |
| and    | 与             | price>9.00 and price<9.90 | 如果 price 是 9.80，则返回 true。如果 price 是 8.50，则返回 false。 |
| mod    | 计算除法的余数 | 5 mod 2                   | 1                                                            |

**示例**

```html
<html>
    <body>
        <div class="class_1">
            <a herf="link.html"></a>
            <ul>
                <li class="">None</li>
            </ul>
        <div>
        <div class="class_2">
			<ul>
                <li class="item-0">
                    <a href="link1.html">first item</a>
                </li>
                <li class="item-1">
                    <a href="link2.html">second item</a>
                </li>
                <li class="item-inactive">
                    <a href="link3.html">third item</a>
                </li>
            </ul>
		</div>
	</body>
</html>
```

**获取第二`div`**

```
//div[@class="class_2"]
```

**在上一个的结果上，获取其第三个`li`标签**

```
.//li[3]
```

**获取所有`li`下的`a`标签**

```
//li//a
```

**获取所有`li`下的`a`标签中的文本**

```
//li//a/text()
```

**获取所有`li`下的`a`标签中的链接(href)**

```
//li//a/@href
```





## 问题解决方案

### cmd中输入python会打开win10的应用商店

在较新的win10版本上会遇到此类问题。将环境变量`path`中的`C:\Users…\AppData\Local\Microsoft\WindowsApps`删除。



### cmd输入pip命令没有效果

pip的环境变量没有配置。在path路径中增添一行，路径指向python文件下的Scripts文件夹中，例如：`C:\Program Files (x86)\Python\Python38\Scripts`

原因：pip没有加入环境变量，所以cmd中输入pip命令不管用。而pip的存放路径在python的Scripts文件夹中，有个`pip.exe`和`pip3.exe`，其中pip是python2使用的，pip3是python3使用的。如果电脑上只有`python3`，则使用`pip`还是`pip3`是没有影响的。



### pip版本升级

```
python -m pip install --upgrade pip -i https://pypi.tuna.tsinghua.edu.cn/simple
```

如果没有后面的链接，更新会很慢，且会产生超时的报错，所以使用国内的源来更新速度就很快了。其他国内镜像源地址有：
**清华：**`https://pypi.tuna.tsinghua.edu.cn/simple`
**阿里云：**`http://mirrors.aliyun.com/pypi/simple/`
**中国科技大学：**`https://pypi.mirrors.ustc.edu.cn/simple/`
**华中理工大学：**`http://pypi.hustunique.com/`
**山东理工大学：**`http://pypi.sdutlinux.org/`
**豆瓣：**`http://pypi.douban.com/simple/`



### 安装BeautifulSoup

```
pip3 install beautifulsoup4 -i https://pypi.tuna.tsinghua.edu.cn/simple
```

**注意：**beautifulsoup后面需要加上4，代表安装的版本，这个包兼容了python2和python3。

更多beautifulsoup的内容访问**官网：**https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#



### 安装BeautifulSoup中的lxml解析器

```
pip install lxml -i https://pypi.tuna.tsinghua.edu.cn/simple
```



### 安装requests

```
pip3 install requests -i https://pypi.tuna.tsinghua.edu.cn/simple
```



### 安装aiohttp

```
pip install aiohttp -i https://pypi.tuna.tsinghua.edu.cn/simple
```

**官网：**https://docs.aiohttp.org/en/stable/index.html



### 安装Selenium

```
pip install selenium -i https://pypi.tuna.tsinghua.edu.cn/simple
```

**浏览器对应的驱动(Chrome)：**https://npm.taobao.org/mirrors/chromedriver

**中文文档：**https://selenium-python-zh.readthedocs.io/en/latest/

驱动存放位置需要写到环境变量的`path`中，如果依旧使用无效，记得重启PyCharm试试。



### 安装Scrapy

```
pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple
```

**中文文档：**https://scrapy-chs.readthedocs.io/zh_CN/0.24/

**英文文档：**https://docs.scrapy.org/en/latest/



### 安装Pillow

如果使用`scrapy`下载图片，则需要先安装`pillow`

```
pip install pillow -i https://pypi.tuna.tsinghua.edu.cn/simple
```



### 正则匹配超链接

```
href = re.search(r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2})|/)+', "https://...").group()
```

